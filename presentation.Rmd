---
title: "Multivariate data analysis: a discriminating method of the origin of wines"
output: 
  html_notebook:
    toc: true
    toc_depth: 2
author: 'Matteo Fasulo, Simone Flavio Paris, Matteo Sivoccia'

---
```{r loadEnv, message=FALSE, warning=FALSE}
source("https://raw.githubusercontent.com/MatteoFasulo/Rare-Earth/main/R/util/coreFunctions.R")

loadPackages(c('sn','tidyverse','car','RColorBrewer','stargazer','mclust','ContaminatedMixt',
               'plotly','ggplot2','ggdendro','pgmm','teigen','tclust','HDMD','caTools','clustvarsel',
               'vscc','sparcl','SelvarMix','pgmm','tidyverse','caret','glmnet','MLmetrics'))

load("C:\\Users\\Matte\\Documents\\LUMSA\\Data Mining\\finite_mixture\\FiniteMixtureL31.RData")
rm(CO2data)
rm(NOdata)
rm(tonedata)
type <- wine$Type
rm(wine)
```
Dati su 27 caratteristiche chimico/fisiche di tre diversi tipi di vino (Barolo, Grignolino, Barbera)
dal Piemonte. Un set di dati con 178 osservazioni e 28 variabili (di cui la prima relativa alla tipologia di vino). Nell'ordine: 

- Barolo 
- Grignolino
- Barbera

E' stato possibile attraverso la ricerca originaria risalire all'anno di osservazione di ciascun vino. Di seguito vengono riportate le osservazioni dei tre diversi tipi di vino durante gli anni:

```{r vinoAnni, message=FALSE, warning=FALSE}
data(wines)
year <- as.numeric(substr(rownames(wines), 6, 7))
table(wines$wine, year)
#wines[,'wine'] <- type
```
Notiamo subito che il Barbera è distribuito principalmente negli ultimi anni (76,78,79) mentre il Barolo nel 71, 73 e 74.
Per quanto riguarda la percentuale delle singole classi:

```{r nClassi, message=FALSE, warning=FALSE}
wines %>%
  count(wine = factor(wine)) %>%
  mutate(pct = prop.table(n)) %>% 
  ggplot(aes(x = wine, y = pct, fill = wine, label = scales::percent(pct))) + 
  geom_col(position = 'dodge') + 
  geom_text(position = position_dodge(width = .9),
            vjust = -0.5, 
            size = 3) + 
  scale_y_continuous(name = "Percentage")+
  scale_x_discrete(name = "Wine Name")+
  scale_fill_hue(l=40, c=35)+
  theme(legend.position = "none")
```
E' chiaro che il Grignolino sia il più numeroso (39.9%) seguito dal Barolo (33.1%)e dal Barbera (27.0%).

Nella *matrix scatterplot*, le celle sulla diagonale mostrano gli istogrammi di ognuna delle variabili, in questo caso le concentrazioni delle prime 10 caratteristiche chimiche. Ognuna delle celle fuori dalla diagonale, rappresenta lo scatterplot di 2 delle 10 caratteristiche.

```{r scatterPlot, message=FALSE, warning=FALSE}
#scatterplotMatrix(wines[2:10])
#scatterplotMatrix(wines[11:20])
#scatterplotMatrix(wines[21:28])
```
Per visualizzare le statistiche descrittive (media e deviazione standard) ci è sembrato opportuno dividerle in base alla classe di appartenenza:
```{r descriptive, message=FALSE, warning=FALSE}
printMeanAndSdByGroup <- function(variables,groupvariable)
  {
     variablenames <- c(names(groupvariable),names(as.data.frame(variables)))
     groupvariable <- groupvariable[,1]
     means <- aggregate(as.matrix(variables) ~ groupvariable, FUN = mean)
     names(means) <- variablenames
     print(paste("Means:"))
     print(means)
     sds <- aggregate(as.matrix(variables) ~ groupvariable, FUN = sd)
     names(sds) <- variablenames
     print(paste("Standard deviations:"))
     print(sds)
}
printMeanAndSdByGroup(wines[2:28],wines[1])
```
Alcune considerazioni :
- La media di sugar, potassium, magnesium, phosphate, chloride, flavanoids, proanthocyanins, colour nel Barolo è più alta.
- La media di acidity, tartaric, malic, uronic, alcal_ash nel Barbera è più alta.
- La deviazione standard di acidity è più alta nel Grignolino. 

Per tali motivi, abbiamo calcolato la varianza within tra una qualunque feature e i tipi di vino:
```{r withinVariance, message=FALSE, warning=FALSE}
calcWithinGroupsVariance <- function(variable,groupvariable)
  {
     groupvariable2 <- as.factor(groupvariable[[1]])
     levels <- levels(groupvariable2)
     numlevels <- length(levels)
     numtotal <- 0
     denomtotal <- 0
     for (i in 1:numlevels)
     {
        leveli <- levels[i]
        levelidata <- variable[groupvariable==leveli,]
        levelilength <- length(levelidata)
        sdi <- sd(levelidata)
        numi <- (levelilength - 1)*(sdi * sdi)
        denomi <- levelilength
        numtotal <- numtotal + numi
        denomtotal <- denomtotal + denomi
     }
     Vw <- numtotal / (denomtotal - numlevels)
     return(Vw)
}
calcWithinGroupsVariance(wines[2],wines[1])
```
Stesso discorso per la varianza between tra una feature e i vini: 
```{r betweenVariance}
calcBetweenGroupsVariance <- function(variable,groupvariable)
  {
     # find out how many values the group variable can take
     groupvariable2 <- as.factor(groupvariable[[1]])
     levels <- levels(groupvariable2)
     numlevels <- length(levels)
     # calculate the overall grand mean:
     grandmean <- sapply(variable,mean)
     # get the mean and standard deviation for each group:
     numtotal <- 0
     denomtotal <- 0
     for (i in 1:numlevels)
     {
        leveli <- levels[i]
        levelidata <- variable[groupvariable==leveli,]
        levelilength <- length(levelidata)
        # get the mean and standard deviation for group i:
        meani <- mean(levelidata)
        sdi <- sd(levelidata)
        numi <- levelilength * ((meani - grandmean)^2)
        denomi <- levelilength
        numtotal <- numtotal + numi
        denomtotal <- denomtotal + denomi
     }
     # calculate the between-groups variance
     Vb <- numtotal / (numlevels - 1)
     Vb <- Vb[[1]]
     return(Vb)
}
calcBetweenGroupsVariance (wines[2],wines[1])
```

```{r distance, echo=FALSE, message=FALSE, warning=FALSE}
dissMatrix    <- as.dist(pairwise.mahalanobis(wines[,-1], grouping = c(1:nrow(wines)), cov = cov(wines[,-1]))$distance)
combDist <- function(distance, methods, df, dt, dissMatrix) {
  c <- 0
  results <- list()
  for (i in 1:length(distance)){
    ifelse(distance[i] == "minkowski",
           dist = dist(df, method = distance[i], p = 3),
           ifelse(distance[i] == "mahalanobis",
                  dist = dissMatrix,
                  dist = dist(df, method = distance[i])))
    for (j in 1:length(methods)){
      dendo = hclust(dist, method = methods[j])
      for(elem in 2:4){
        cluster = cutree(dendo, k=elem)
        c <- c + 1
        results[[c]] <- list(distance = distance[i],
                             method = methods[j],
                             groups = elem,
                             table = table(dt,cluster),
                             AdjustedRandIndex = adjustedRandIndex(dt,cluster),
                             cluster = cluster)
      }
    }
  }
  return(results)
}
results <- combDist(c("euclidean", "manhattan", "minkowski","mahalanobis"),
                    c("single", "complete", "average", "ward.D"), scale(wines[,-1]), wines[,1], dissMatrix)

optimal <- function(results){
  best_randIndex = 0
  best_model = integer()
  for (i in 1:length(results)){
    current_randIndex = results[[i]]$AdjustedRandIndex
    if (current_randIndex > best_randIndex) {
      best_randIndex = current_randIndex
      best_model = i
    }
  }
  print(list(best_randIndex=best_randIndex,
             best_model=best_model))
  return(results[[best_model]])
}
best_dist_model = optimal(results)

ggplotly(ggdendrogram(best_dist_model, rotate = F, size = 2, leaf_labels = T, labels = F))
```

```{r splitTrainTest, message=FALSE, warning=FALSE}
require(caTools)
sample = sample.split(wines[,1], SplitRatio = .50)

train = subset(wines, sample == TRUE)
trainTestNames <- train$wine
#train$wine <- as.numeric(train$wine)

test  = subset(wines, sample == FALSE)
wineTestNames <- test$wine
#test$wine <- as.numeric(test$wine)
```

```{r partitioning, message=FALSE, warning=FALSE}
require(cluster)
k.means.3 <- kmeans(scale(wines[,-1]),centers=3,nstart = 50, iter.max = 100)
k.means.4 <- kmeans(scale(wines[,-1]),centers=4,nstart = 50, iter.max = 100)

PAM.3 <- pam(wines[,-1], k=3,
    metric = "euclidean", 
    nstart = 50,
    stand = TRUE)
PAM.4 <- pam(wines[,-1], k=4,
    metric = "euclidean", 
    nstart = 50,
    stand = TRUE)
```

```{r modelBased, message=FALSE, warning=FALSE}
mixt.wines <- Mclust(wines[,-1],G=3:8)

cn.wines.mixt <- CNmixt(wines[,-1], G = 3, initialization = "mixt", seed = 1234, parallel = F, verbose = F)
cn.wines.kmeans <- CNmixt(wines[,-1], G = 3, initialization = "kmeans", seed = 1234, parallel = F, verbose = F)
cn.wines.rpost <- CNmixt(wines[,-1], G = 3, initialization = "random.post", seed = 1234, parallel = F, verbose = F)
cn.wines.rclass <- CNmixt(wines[,-1], G = 3, initialization = "random.clas", seed = 1234, parallel = F, verbose = F)


teigen.kmeans <- teigen(wines[,-1], Gs=3:4, init = 'kmeans', scale = T)
adjustedRandIndex(wines[,1],teigen.kmeans$classification)

teigen.classifier.kmeans <- teigen(train[,-1], Gs=3:4, init = 'kmeans', scale = T, known = train[,1])
predict(teigen.classifier.kmeans,test[,-1])
adjustedRandIndex(test[,1],predict(teigen.classifier.kmeans, test[,-1])$classification)

teigen.classifier.uniform <- teigen(train[,-1], Gs=3:4, init = 'uniform', scale = T, known = train[,1])
predict(teigen.classifier.uniform,test[,-1])
adjustedRandIndex(test[,1],predict(teigen.classifier.uniform, test[,-1])$classification)

teigen.hard <- teigen(wines[,-1], Gs=3:4, init = 'hard', scale = T)
adjustedRandIndex(wines[,1],teigen.hard$classification)
teigen.soft <- teigen(wines[,-1], Gs=3:4, init = 'soft', scale = T)
adjustedRandIndex(wines[,1],teigen.soft$classification)
```

```{r varSelection, message=FALSE, warning=FALSE}
subset.headlong <- clustvarsel(wines[,-1],G=3:4, search = 'headlong', direction = 'forward', parallel = T)
subset.greedy <- clustvarsel(wines[,-1],G=3:4, search = 'greedy', direction = 'forward', parallel = T)

headlong.selected <- subset.headlong$model
table(wines[,1],headlong.selected$classification)
greedy.selected <- subset.greedy$model
table(wines[,1],greedy.selected$classification)

vscc.mclust <- vscc(wines[,-1], G=3:4, automate = "mclust", initial = NULL, train = NULL, forcereduction = FALSE)
table(wines[,1], vscc.mclust$initialrun$classification) #Clustering results on full data set
table(wines[,1], vscc.mclust$bestmodel$classification) #Clustering results on reduced data set

km.perm.3 <- KMeansSparseCluster.permute(wines[,-1],K=3,wbounds=seq(3,7,len=15),nperms=50,silent = T)
km.perm.4 <- KMeansSparseCluster.permute(wines[,-1],K=4,wbounds=seq(3,7,len=15),nperms=50,silent = T)
# run sparse k-means
km.sparse.3 <- KMeansSparseCluster(wines[,-1],K=3,wbounds=km.perm.3$bestw,nstart = 50, silent = T, maxiter=100)
table(wines[,1],km.sparse.3[[1]]$Cs)
km.sparse.4 <- KMeansSparseCluster(wines[,-1],K=4,wbounds=km.perm.4$bestw,nstart = 50, silent = T, maxiter=100)
table(wines[,1],km.sparse.4[[1]]$Cs)
#
rho <- seq(1, 2, length=2)
lasso <- SelvarClustLasso(wines[,-1], nbcluster=3:4,criterion="ICL",lambda=lambda,rho=rho,nbcores = 6)
summary(lasso)
lasso$S
lasso$R
lasso$U
lasso$nbcluster
table(wines[,1],lasso$partition)
```
## Tecniche di regolarizzazione
Per la nostra analisi abbiamo voluto verificare l'efficienza di tre noti modelli di regolarizzazione attraverso il pacchetto *caret*:

- Ridge
- Lasso
- Elastic Net

```{r lambdaTuning, message=FALSE, warning=FALSE}
lambda <- 10^seq(.001, .1, by = .001)
```

### Ridge
Il modello *Ridge* riduce i coefficienti, in modo che le variabili, con un contributo minore al risultato, abbiano i loro coefficienti vicini allo zero. Invece di forzarli a essere esattamente zero (come nel *Lasso*), li penalizziamo con un termine chiamato *norma L2* costringendoli così a essere piccoli. In questo modo diminuiamo la complessità del modello senza eliminare nessuna variabile attraverso una costante chiamata lambda ($\lambda$) di penalizzazione:
$$
L_{ridge}(\hat{\beta}) = \sum_{i = 1}^{n}{(y_i - x_i\hat{\beta})^2} + \lambda\sum_{k = 1}^{K}{\hat{\beta}_k^2}
$$

```{r caretRidge, message=FALSE, warning=FALSE}
# Build the model
set.seed(123)
ridge <- caret::train(
  x = train[,-1],
  y = factor(train[,1]),
  method = "glmnet",
  trControl = trainControl("cv", number = 10, classProbs = TRUE, summaryFunction = multiClassSummary),
  tuneGrid = expand.grid(alpha = 0),
  metric="ROC")
# Model coefficients
coef(ridge$finalModel, ridge$bestTune$lambda)
# Make predictions
predictions <- ridge %>% predict(test)
# Model prediction performance
tibble(
  trueValue = wineTestNames,
  predictedValue = predictions)
```
```{r ridgeResult}
caret::confusionMatrix(predictions, test$wine)
```
Il ridge è composto dalla somma dei residui quadrati più una penalità, definita dalla lettera Lambda, che è moltiplicata per la somma dei coefficienti quadrati $\beta$. Quando $\lambda = 0$, il termine di penalità non ha alcun effetto e il ridge produrrà i coefficienti minimi quadrati classici. Tuttavia, quando $\lambda$ aumenta all’infinito, l’impatto della penalità aumenta e i coefficienti si avvicinano allo zero. Il ridge è particolarmente indicato quando si hanno molti dati multivariati con numero di feature maggiore del numero di osservazioni. Lo svantaggio, però, è che includerà tutti le feature nel modello finale, a differenza dei metodi di feature selection, che generalmente selezioneranno un insieme ridotto di variabili tra quelle disponibili.

### Lasso
Il *Least Absolute Shrinkage and Selection Operator* (LASSO) riduce i coefficienti verso lo zero penalizzando il modello con un termine di penalità chiamato *norma L1*, che è la somma dei coefficienti in valore assoluto:
$$
L_{lasso}(\hat{\beta}) = \sum_{i = 1}^{n}{(y_i - x_i\hat{\beta})^2} + \lambda\sum_{k = 1}^{K}{|\hat{\beta}_k|}
$$
```{r caretLasso, message=FALSE, warning=FALSE}
# Build the model
set.seed(123)
lasso <- caret::train(
  wine ~., data = train, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda)
  )
# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
# Make predictions
predictions <- lasso %>% predict(test)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test$wine),
  Rsquare = R2(predictions, test$wine)
)
```
In questo caso la penalità ha l’effetto di forzare alcune delle stime dei coefficienti, con un contributo minore al modello, a essere esattamente uguale a zero. Il lasso, quindi, può anche essere visto come un’alternativa ai metodi di feature selection per eseguire la selezione delle variabili al fine di ridurre la complessità del modello.

Come nel ridge, è fondamentale selezionare un buon valore di $\lambda$.

Quando lambda è piccolo, il risultato è molto vicino alla stima dei minimi quadrati. All’aumentare di lambda, si verifica una contrazione in modo da poter eliminare le variabili che sono a zero.

### Elastic Net
Elastic Net combina le proprietà di Ridge e Lasso penalizzando il modello usando sia la norma L2 che la norma L1:
$$
L_{E    lasticNet}(\hat{\beta}) = \frac{\sum_{i = 1}^{n}{(y_i - x_i\hat{\beta})^2}}{2n} + \lambda(\frac{1-\alpha}{2}\sum_{k = 1}^{K}{\hat{\beta}_k^2} + \alpha\sum_{k = 1}^{K}{|\hat{\beta}_k}|)
$$
```{r caretElastic, message=FALSE, warning=FALSE}
set.seed(123)
elastic <- caret::train(
  wine ~., data = train, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
  )
# Model coefficients
coef(elastic$finalModel, elastic$bestTune$lambda)
# Make predictions
predictions <- elastic %>% predict(test)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test$wine),
  Rsquare = R2(predictions, test$wine)
)
models <- list(ridge = ridge, lasso = lasso, elastic = elastic)
resamples(models) %>% summary(metric = "RMSE")
```
Oltre a impostare e scegliere un valore lambda, l’*elastic net* ci consente anche di ottimizzare il parametro alfa dove $\alpha = 0$ corrisponde a *ridge* e $\alpha = 1$ al *lasso*.

Pertanto possiamo scegliere un valore $\alpha$ compreso tra 0 e 1 per ottimizzare l’elastic net. Se tale valore è incluso in questo intervallo, si avrà una riduzione con alcuni portati a $0$.

```{r advanced, message=FALSE, warning=FALSE}
# Il rilassamento dato da (p-q)^2 > p+q è verificato per  1 <= q <= 20
pg.random.ccc <- pgmmEM(selectedWines[,-1], rG=3:4, rq=1:5, seed = 1234, relax = T)
adjustedRandIndex(wines[,1],pg.random.ccc$map)

pg.random <- pgmmEM(wines[,-1], rG=3:4, rq=20:25,icl=F,zstart = 1,cccStart = F,loop = 50, seed = 1234, relax = T)
adjustedRandIndex(wines[,1],pg.random.ccc$map)

pg2 <- pgmmEM(wines[,-1], rG=3:4, rq=20:25,icl=T,zstart = 1,cccStart = T,loop = 50, seed = 1234, relax = T)
adjustedRandIndex(wines[,1],pg2$map)

pg2 <- pgmmEM(wines[,-1], rG=3:4, rq=20:25,icl=T,zstart = 2, seed = 1234, relax = T)
adjustedRandIndex(wines[,1],pg2$map)

pg2 <- pgmmEM(wines[,-1], rG=3:4, rq=20:25,icl=T,zstart = 3, seed = 1234, relax = T, zlist=)
adjustedRandIndex(wines[,1],pg2$map)




agree(cn.wines.mixt, givgroup = wines[,1])
agree(cn.wines.kmeans, givgroup = wines[,1])
agree(cn.wines.rpost, givgroup = wines[,1])
agree(cn.wines.rclass, givgroup = wines[,1])

plot(cn.wines.mixt, contours = TRUE)
plot(cn.wines.kmeans, contours = TRUE)
plot(cn.wines.rpost, contours = TRUE)
plot(cn.wines.rclass, contours = TRUE)
```

if (class(models[[i]])=='kmeans'){
      ariTrain[i] <- adjustedRandIndex(train,models[[i]]$cluster)
      ariTest[i] <- adjustedRandIndex(test,models[[i]]$cluster)
      bic[i] <- NA
      icl[i] <- NA
      G[i] <- length(unique(models[[i]]$cluster))


```{r summaryOfAllModels, message=FALSE, warning=FALSE}
summaryOfModels <- function(train, test, models){
  nModel <- length(models)
  ariTrain <- NULL
  ariTest <- NULL
  bic <- NULL
  icl <- NULL
  G <- NULL
  for(i in 1:nModel){
           if (class(models[[i]])=='Mclust'){
      ariTrain[i] <- adjustedRandIndex(train,models[[i]]$classification)
      ariTest[i] <- adjustedRandIndex(test,predict(models[[i]],test)$classification)
      bic[i] <- models[[i]]$bic
      icl[i] <- models[[i]]$icl
      G[i] <- models[[i]]$G
    } else if (class(models[[i]])=='ContaminatedMixt'){
      bestModel <- getBestModel(models[[i]])$models[[1]]
      ariTrain[i] <- adjustedRandIndex(train,bestModel$group)
      ariTest[i] <- adjustedRandIndex(test,predict(bestModel,))
      bic[i] <- whichBestModel(models[[i]],train)$BIC
      icl[i] <- whichBestModel(models[[i]],train)$ICL
      G[i] <- whichBestModel(models[[i]],train)$G
    } else if (class(models[[i]])=='teigen'){
      ariTrain[i] <- adjustedRandIndex(train,models[[i]]$iclresults$classification)
      ariTest[i] <- adjustedRandIndex(test,models[[i]]$iclresults$classification)
      bic[i] <- models[[i]]$bic
      icl[i] <- models[[i]]$iclresults$icl
      G[i] <- models[[i]]$G
    } else if (class(models[[i]])=='pgmm'){
      ari[i] <- adjustedRandIndex(df,models[[i]]$map)
      G[i] <- models[[i]]$g
      ifelse(is.null(models[[i]]$bic[1])==TRUE,
             bic[i] <- NA,
             bic[i] <- as.double(models[[i]]$bic[1]))
      ifelse(is.null(models[[i]]$icl[1])==TRUE,
             icl[i] <- NA,
             icl[i] <- as.double(models[[i]]$icl[1]))
    } else if (class(models[[i]])=='tkmeans'){
      ari[i] <- adjustedRandIndex(df,models[[i]]$cluster)
      G[i] <- models[[i]]$k
      bic[i] <- NA
      icl[i] <- NA
    }
  }
  outputDF <- data.frame('AdjustedRandIndex' = ari,
                         'BIC' = bic,
                         'ICL' = icl,
                         'G' = as.integer(G),
                         row.names = c('KMeans','Mclust','ContaminatedMixt','TEigen','PGMM','TClust'),
                         stringsAsFactors = F)
  return(outputDF)
}
test <- summaryOfModels(wines[,1],list(k.means,mixt.wines,mixt.cn.wines,teigen_wine,pippo,trimmedOne))
test
```
