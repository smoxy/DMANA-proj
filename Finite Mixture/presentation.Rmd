---
title: "Multivariate data analysis: a discriminating method of the origin of wines"
output: html_notebook
author: 'Matteo Fasulo, Simone Flavio Paris, Matteo Sivoccia'
---
```{r loadEnv, message=FALSE, warning=FALSE}
source("https://raw.githubusercontent.com/MatteoFasulo/Rare-Earth/main/R/util/coreFunctions.R")
loadPackages(c('sn','tidyverse','car','RColorBrewer','stargazer','mclust','ContaminatedMixt',
               'plotly','ggplot2','ggdendro','pgmm','teigen','tclust','HDMD','caTools','clustvarsel'))
load("Z:\\DesktopC\\LUMSA\\2\\Data Mining\\Finite Mixture\\FiniteMixtureL31.RData")
rm(CO2data)
rm(NOdata)
rm(tonedata)
type <- wine$Type
rm(wine)
```
Dati su 27 caratteristiche chimico/fisiche di tre diversi tipi di vino (Barolo, Grignolino, Barbera)
dal Piemonte. Un set di dati con 178 osservazioni e 28 variabili (di cui la prima relativa alla tipologia di vino). Nell'ordine: 

- Barolo 
- Grignolino
- Barbera

E' stato possibile attraverso la ricerca originaria risalire all'anno di osservazione di ciascun vino. Di seguito vengono riportate le osservazioni dei tre diversi tipi di vino durante gli anni:

```{r vinoAnni, message=FALSE, warning=FALSE}
data(wines)
year <- as.numeric(substr(rownames(wines), 6, 7))
table(wines$wine, year)
```

Per quanto riguarda la numerosità delle singole classi:

```{r nClassi, message=FALSE, warning=FALSE}
wines %>%
  count(wine = factor(wine)) %>%
  mutate(pct = prop.table(n)) %>% 
  ggplot(aes(x = wine, y = pct, fill = wine, label = scales::percent(pct))) + 
  geom_col(position = 'dodge') + 
  geom_text(position = position_dodge(width = .9),
            vjust = -0.5, 
            size = 3) + 
  scale_y_continuous(name = "Percentage")+
  scale_x_discrete(name = "Wine Name")+
  scale_fill_hue(l=40, c=35)+
  theme(legend.position = "none")
```


```{r scatterPlot, message=FALSE, warning=FALSE}
wines[,'wine'] <- type # fix wine name in numeric
scatterplotMatrix(wines[2:10])
scatterplotMatrix(wines[11:20])
scatterplotMatrix(wines[21:28])
```
Nella *matrix scatterplot*, le celle sulla diagonale mostrano gli istogrammi di ognuna delle variabili, in questo caso le concentrazioni delle prime 10 caratteristiche chimiche. Ognuna delle celle fuori dalla diagonale, rappresenta lo scatterplot di 2 delle 10 caratteristiche.
```{r descriptive, message=FALSE, warning=FALSE}
printMeanAndSdByGroup <- function(variables,groupvariable)
  {
     variablenames <- c(names(groupvariable),names(as.data.frame(variables)))
     groupvariable <- groupvariable[,1]
     means <- aggregate(as.matrix(variables) ~ groupvariable, FUN = mean)
     names(means) <- variablenames
     print(paste("Means:"))
     print(means)
     sds <- aggregate(as.matrix(variables) ~ groupvariable, FUN = sd)
     names(sds) <- variablenames
     print(paste("Standard deviations:"))
     print(sds)
}
printMeanAndSdByGroup(wines[2:28],wines[1])
```

```{r withinVariance, message=FALSE, warning=FALSE}
calcWithinGroupsVariance <- function(variable,groupvariable)
  {
     # find out how many values the group variable can take
     groupvariable2 <- as.factor(groupvariable[[1]])
     levels <- levels(groupvariable2)
     numlevels <- length(levels)
     # get the mean and standard deviation for each group:
     numtotal <- 0
     denomtotal <- 0
     for (i in 1:numlevels)
     {
        leveli <- levels[i]
        levelidata <- variable[groupvariable==leveli,]
        levelilength <- length(levelidata)
        # get the standard deviation for group i:
        sdi <- sd(levelidata)
        numi <- (levelilength - 1)*(sdi * sdi)
        denomi <- levelilength
        numtotal <- numtotal + numi
        denomtotal <- denomtotal + denomi
     }
     # calculate the within-groups variance
     Vw <- numtotal / (denomtotal - numlevels)
     return(Vw)
}
calcWithinGroupsVariance(wines[2],wines[1])
```
```{r betweenVariance}
calcBetweenGroupsVariance <- function(variable,groupvariable)
  {
     # find out how many values the group variable can take
     groupvariable2 <- as.factor(groupvariable[[1]])
     levels <- levels(groupvariable2)
     numlevels <- length(levels)
     # calculate the overall grand mean:
     grandmean <- sapply(variable,mean)
     # get the mean and standard deviation for each group:
     numtotal <- 0
     denomtotal <- 0
     for (i in 1:numlevels)
     {
        leveli <- levels[i]
        levelidata <- variable[groupvariable==leveli,]
        levelilength <- length(levelidata)
        # get the mean and standard deviation for group i:
        meani <- mean(levelidata)
        sdi <- sd(levelidata)
        numi <- levelilength * ((meani - grandmean)^2)
        denomi <- levelilength
        numtotal <- numtotal + numi
        denomtotal <- denomtotal + denomi
     }
     # calculate the between-groups variance
     Vb <- numtotal / (numlevels - 1)
     Vb <- Vb[[1]]
     return(Vb)
}
calcBetweenGroupsVariance (wines[2],wines[1])
```

```{r distance, echo=FALSE, message=FALSE, warning=FALSE}
dissMatrix    <- as.dist(pairwise.mahalanobis(wines[,-1], grouping = c(1:nrow(wines)), cov = cov(wines[,-1]))$distance)
combDist <- function(distance, methods, df, dt, dissMatrix) {
  c <- 0
  results <- list()
  for (i in 1:length(distance)){
    ifelse(distance[i] == "minkowski",
           dist = dist(df, method = distance[i], p = 3),
           ifelse(distance[i] == "mahalanobis",
                  dist = dissMatrix,
                  dist = dist(df, method = distance[i])))
    for (j in 1:length(methods)){
      dendo = hclust(dist, method = methods[j])
      for(elem in 2:4){
        cluster = cutree(dendo, k=elem)
        c <- c + 1
        results[[c]] <- list(distance = distance[i],
                             method = methods[j],
                             groups = elem,
                             table = table(dt,cluster),
                             AdjustedRandIndex = adjustedRandIndex(dt,cluster),
                             cluster = cluster)
      }
    }
  }
  return(results)
}
results <- combDist(c("euclidean", "manhattan", "minkowski","mahalanobis"),
                    c("single", "complete", "average", "ward.D"), scale(wines[,-1]), wines[,1], dissMatrix)

optimal <- function(results){
  best_randIndex = 0
  best_model = integer()
  for (i in 1:length(results)){
    current_randIndex = results[[i]]$AdjustedRandIndex
    if (current_randIndex > best_randIndex) {
      best_randIndex = current_randIndex
      best_model = i
    }
  }
  print(list(best_randIndex=best_randIndex,
             best_model=best_model))
  return(results[[best_model]])
}
best_dist_model = optimal(results)

ggplotly(ggdendrogram(best_dist_model, rotate = F, size = 2, leaf_labels = T, labels = F))
```

```{r splitTrainTest, message=FALSE, warning=FALSE}
require(caTools)
sample = sample.split(wines[,1], SplitRatio = .75)
train = subset(wines, sample == TRUE)
test  = subset(wines, sample == FALSE)
```

```{r partitioning, message=FALSE, warning=FALSE}
k.means.3 <- kmeans(scale(wines[,-1]),centers=3,nstart = 50, iter.max = 100)
k.means.4 <- kmeans(scale(wines[,-1]),centers=4,nstart = 50, iter.max = 100)

PAM.3
PAM.4
```

```{r modelBased, message=FALSE, warning=FALSE}
mixt.wines <- Mclust(wines[,-1],G=3:8)

cn.wines.mixt <- CNmixt(wines[,-1], G = 3, initialization = "mixt", seed = 1234, parallel = F, verbose = F)
cn.wines.kmeans <- CNmixt(wines[,-1], G = 3, initialization = "kmeans", seed = 1234, parallel = F, verbose = F)
cn.wines.rpost <- CNmixt(wines[,-1], G = 3, initialization = "random.post", seed = 1234, parallel = F, verbose = F)
cn.wines.rclass <- CNmixt(wines[,-1], G = 3, initialization = "random.clas", seed = 1234, parallel = F, verbose = F)


teigen.kmeans <- teigen(wines[,-1], Gs=3:4, init = 'kmeans', scale = T)
adjustedRandIndex(wines[,1],teigen.kmeans$classification)

teigen.classifier.kmeans <- teigen(train[,-1], Gs=3:4, init = 'kmeans', scale = T, known = train[,1])
predict(teigen.classifier.kmeans,test[,-1])
adjustedRandIndex(test[,1],predict(teigen.classifier.kmeans, test[,-1])$classification)

teigen.classifier.uniform <- teigen(train[,-1], Gs=3:4, init = 'uniform', scale = T, known = train[,1])
predict(teigen.classifier.uniform,test[,-1])
adjustedRandIndex(test[,1],predict(teigen.classifier.uniform, test[,-1])$classification)

teigen.hard <- teigen(wines[,-1], Gs=3:4, init = 'hard', scale = T)
adjustedRandIndex(wines[,1],teigen.hard$classification)
teigen.soft <- teigen(wines[,-1], Gs=3:4, init = 'soft', scale = T)
adjustedRandIndex(wines[,1],teigen.soft$classification)
```

```{r varSelection, message=FALSE, warning=FALSE}
subset.headlong <- clustvarsel(wines[,-1],G=3:8, search = 'headlong', direction = 'forward', parallel = T)
subset.greedy <- clustvarsel(wines[,-1],G=3:8, search = 'greedy', direction = 'forward', parallel = T)

headlong.selected <- subset.headlong$model
greedy.selected <- subset.greedy$model

vscc.mclust <- vscc(wines[,-1], G=3:8, automate = "mclust", initial = NULL, train = NULL, forcereduction = FALSE)
table(wines[,1], vscc.mclust$initialrun$classification) #Clustering results on full data set
table(wines[,1], vscc.mclust$bestmodel$classification) #Clustering results on reduced data set

km.perm <- KMeansSparseCluster.permute(wines[,-1],K=3,wbounds=seq(3,7,len=15),nperms=50,silent = T)
print(km.perm)
plot(km.perm)
# run sparse k-means
km.out <- KMeansSparseCluster(wines[,-1],K=3,wbounds=km.perm$bestw,nstart = 50, silent = T, maxiter=100)
print(km.out)
plot(km.out)
table(km.out[[1]]$Cs,wines[,1])
#
lambda <- 10^seq(-3, 3, length = 100)
rho <- seq(1, 2, length=2)
lasso <- SelvarClustLasso(wines[,-1], nbcluster=3:4,criterion="ICL",lambda=lambda,rho=rho,nbcores = 2)
summary(lasso)
lasso$S
lasso$R
lasso$U
lasso$nbcluster
table(wines[,1],lasso$partition)

# Build the model
set.seed(123)
ridge <- train(
  wine ~., data = train, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 0, lambda = lambda)
  )
# Model coefficients
coef(ridge$finalModel, ridge$bestTune$lambda)
# Make predictions
predictions <- ridge %>% predict(test)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test$wine),
  Rsquare = R2(predictions, test$wine)
)

# Build the model
set.seed(123)
lasso <- train(
  wine ~., data = train, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda)
  )
# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
# Make predictions
predictions <- lasso %>% predict(test)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test$wine),
  Rsquare = R2(predictions, test$wine)
)

set.seed(123)
elastic <- train(
  wine ~., data = train, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
  )
# Model coefficients
coef(elastic$finalModel, elastic$bestTune$lambda)
# Make predictions
predictions <- elastic %>% predict(test)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test$wine),
  Rsquare = R2(predictions, test$wine)
)
models <- list(ridge = ridge, lasso = lasso, elastic = elastic)
resamples(models) %>% summary( metric = "RMSE")
```

```{r advanced, message=FALSE, warning=FALSE}
# Il rilassamento dato da (p-q)^2 > p+q è verificato per  1 <= q <= 20
pg.random.ccc <- pgmmEM(selectedWines[,-1], rG=3:8, rq=1:5, seed = 1234, relax = T)
adjustedRandIndex(wines[,1],pg.random.ccc$map)

pg.random <- pgmmEM(wines[,-1], rG=3:4, rq=20:25,icl=F,zstart = 1,cccStart = F,loop = 50, seed = 1234, relax = T)
adjustedRandIndex(wines[,1],pg.random.ccc$map)

pg2 <- pgmmEM(wines[,-1], rG=3:4, rq=20:25,icl=T,zstart = 1,cccStart = T,loop = 50, seed = 1234, relax = T)
adjustedRandIndex(wines[,1],pg2$map)

pg2 <- pgmmEM(wines[,-1], rG=3:4, rq=20:25,icl=T,zstart = 2, seed = 1234, relax = T)
adjustedRandIndex(wines[,1],pg2$map)

pg2 <- pgmmEM(wines[,-1], rG=3:4, rq=20:25,icl=T,zstart = 3, seed = 1234, relax = T, zlist=)
adjustedRandIndex(wines[,1],pg2$map)




agree(cn.wines.mixt, givgroup = wines[,1])
agree(cn.wines.kmeans, givgroup = wines[,1])
agree(cn.wines.rpost, givgroup = wines[,1])
agree(cn.wines.rclass, givgroup = wines[,1])

plot(cn.wines.mixt, contours = TRUE)
plot(cn.wines.kmeans, contours = TRUE)
plot(cn.wines.rpost, contours = TRUE)
plot(cn.wines.rclass, contours = TRUE)
```

if (class(models[[i]])=='kmeans'){
      ariTrain[i] <- adjustedRandIndex(train,models[[i]]$cluster)
      ariTest[i] <- adjustedRandIndex(test,models[[i]]$cluster)
      bic[i] <- NA
      icl[i] <- NA
      G[i] <- length(unique(models[[i]]$cluster))


```{r summaryOfAllModel, message=FALSE, warning=FALSE}
summaryOfModels <- function(train, test, models){
  nModel <- length(models)
  ariTrain <- NULL
  ariTest <- NULL
  bic <- NULL
  icl <- NULL
  G <- NULL
  for(i in 1:nModel){
           if (class(models[[i]])=='Mclust'){
      ariTrain[i] <- adjustedRandIndex(train,models[[i]]$classification)
      ariTest[i] <- adjustedRandIndex(test,predict(models[[i]],test)$classification)
      bic[i] <- models[[i]]$bic
      icl[i] <- models[[i]]$icl
      G[i] <- models[[i]]$G
    } else if (class(models[[i]])=='ContaminatedMixt'){
      bestModel <- getBestModel(models[[i]])$models[[1]]
      ariTrain[i] <- adjustedRandIndex(train,bestModel$group)
      ariTest[i] <- adjustedRandIndex(test,predict(bestModel,))
      bic[i] <- whichBestModel(models[[i]],train)$BIC
      icl[i] <- whichBestModel(models[[i]],train)$ICL
      G[i] <- whichBestModel(models[[i]],train)$G
    } else if (class(models[[i]])=='teigen'){
      ariTrain[i] <- adjustedRandIndex(train,models[[i]]$iclresults$classification)
      ariTest[i] <- adjustedRandIndex(test,models[[i]]$iclresults$classification)
      bic[i] <- models[[i]]$bic
      icl[i] <- models[[i]]$iclresults$icl
      G[i] <- models[[i]]$G
    } else if (class(models[[i]])=='pgmm'){
      ari[i] <- adjustedRandIndex(df,models[[i]]$map)
      G[i] <- models[[i]]$g
      ifelse(is.null(models[[i]]$bic[1])==TRUE,
             bic[i] <- NA,
             bic[i] <- as.double(models[[i]]$bic[1]))
      ifelse(is.null(models[[i]]$icl[1])==TRUE,
             icl[i] <- NA,
             icl[i] <- as.double(models[[i]]$icl[1]))
    } else if (class(models[[i]])=='tkmeans'){
      ari[i] <- adjustedRandIndex(df,models[[i]]$cluster)
      G[i] <- models[[i]]$k
      bic[i] <- NA
      icl[i] <- NA
    }
  }
  outputDF <- data.frame('AdjustedRandIndex' = ari,
                         'BIC' = bic,
                         'ICL' = icl,
                         'G' = as.integer(G),
                         row.names = c('KMeans','Mclust','ContaminatedMixt','TEigen','PGMM','TClust'),
                         stringsAsFactors = F)
  return(outputDF)
}
test <- summaryOfModels(wines[,1],list(k.means,mixt.wines,mixt.cn.wines,teigen_wine,pippo,trimmedOne))
test
```
